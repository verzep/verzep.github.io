---
layout: post
title: "Learning a Dynamical System using its Past: Takens's Theorem"
tags: dynamical_systems, neural_networks
category: machine_learning
---
## Stating the Problem

Let us consider a dynamical system, which we write of the form:

\begin{equation}
 \vec{s}_{t} &= \vec{g}(\vec{s}_{t-1}) \label{eqn:takens_source}
\end{equation}.

For example, we can use the Lorenz system as in the previous example.

```python

# Define the Lorenz system of differential equations
def lorenz(t, y, sigma, rho, beta):
    dydt = [sigma * (y[1] - y[0]),
            y[0] * (rho - y[2]) - y[1],
            y[0] * y[1] - beta * y[2]]
    return dydt

# Set the parameters
sigma = 10.0
rho = 28.0
beta = 8.0 / 3.0

# Set the initial conditions
initial_conditions = [1.0, 0.0, 20.0]

# Set the time span for integration
t_span = (0, 100)
t_eval = np.linspace(t_span[0], t_span[1], 12000)

# Solve the system of differential equations
sol = solve_ivp(lorenz, t_span, initial_conditions, args=(sigma, rho, beta), t_eval=t_eval)
```

We can now plot one orbit and recognize the famous attractor.

%%%% FIGURE LORENZ ATTRACTOR

## Observables
In many applications, it happens that we do not have access to the full state of the system, but only to an observation of its state.

\begin{equation}
 \vec{u}_{t} &= \vec{h}(\vec{s}_{t}) \label{eqn:takens_observable}
\end{equation}.

where we call $u_t$ the _observable_.  Each orbit of \eqref{eqn:takens_source} $\{ \vec{s}_t \}$ corresponds to a time realization of \eqref{eqn:takens_observable} $\{ \vec{u}_t \}$.
In this post, we consider the symple case in which $u_t$ is simply one of the coordinates ($x$) of the Lorenz System.

```python
# Extract the x-coordinate (u)
u = sol.y[0]

#create train and test
u_train = u[500:10000] # we discard some initial points
u_test = u[10000:]
```
In this case, the data at our disposal looks like this:

%%%%% FIGURE LORENZ 1D


## TAKENS'S THEOREM

Since \eqref{eqn:takens_source} is an autonomous system, the initial condition $\vec{s}_{t_{0}}$ determines the entire evolution of the system and, consequently, the entire realization of the observable. 
This means that the entire sequence $\{ \vec{u}_t \}$ depends on the initial condition, in the sense that different initial conditions will lead to different state sequences. 
We now wonder whether the opposite is true or not, i.e., if one can reconstruct the state of the system by observing a series of one-dimensional measures like \eqref{eqn:takens_observable}.  
The Takens's theorem deals with this question.
In order to state the theorem, we now introduce a \emph{delay vector} $\vec{x}_t$ constructed from the observation of our dynamical system as:
    
    \begin{equation}\label{eqn:takens_function}
        \vec{x}_t=
            \begin{bmatrix}
            x^1_t \\
            x^2_t\\
            \dots \\
            x^D_t
        \end{bmatrix}
        =
        \begin{bmatrix}
            u_t \\
            u_{t-1}\\
            \dots \\
            u_{t-(D-1)}
        \end{bmatrix}
        =
        \begin{bmatrix}
            h(\vec{s}_t) \\
            h(\vec{s}_{t-1})\\
            \dots \\
            h(\vec{s}_{t-(D-1)})
        \end{bmatrix}
        =: \vec{F}_D(\vec{s}_t)
    \end{equation}
    

where $\vec{F}_D: \mathbb{R}^d \to \mathbb{R}^D$ is the _$D$-delay map_. Its smoothness depends on the smoothness of $\vec{g}$ and $h$.

If we assume that the motion of \eqref{eqn:takens_source} occurs in manifold $\mathcal{M}$ of dimension $d$, we call $\mathcal{H}_D \subset \mathbb{R}^D$ its image under the effect of $\vec{F}_D$. 
So, we can now present a different formulation of the question above: is $\vec{F}_D$ a diffeormorphism?
The answer to this question in given by the Takens's Embedding Theorem, which we now state (in a fairly informal version):


----------------------  
#### Takens's Embedding
    Let $\mathcal{M}$ be a compact $d$-dimensional $C^2$ manifold.
    For almost any pair of function $\vec{g}$ and $h$, which are continuously differentiable on $\mathcal{M}$, the mapping 
    $$
    \vec{F}_{D}: \mathcal{M} \to \mathbb{R}^{D}
    $$
    given by \eqref{eqn:takens_function} is a diffeomorphism for almost any  $D > 2d$. 
------------------------------------    

Let's code this operation:

```python
# Create a delay embedding
def delay_embedding(data, d, tau):
    N = len(data)
    indices = np.arange(d) * tau + np.arange(N - (d - 1) * tau)[:, None]
    embedded_data = data[indices]
    return embedded_data
```

For example:
```
# Here's an example 
arr = np.array([1,2,3,4,5,6,7,8,9])
emb = delay_embedding(arr,4,2)
emb
```

Will lead to this output

```
array([[1, 3, 5, 7],
       [2, 4, 6, 8],
       [3, 5, 7, 9]])
```

We can apply this function to create the delay embadding of our data:

```python
# Set parameters for delay embedding
d = 20  # Number of delays
tau = 1  # Time delay

# Create delay embedding
embedded_data = delay_embedding(u_train, d, tau)
```


```python
# Perform Singular Value Decomposition (SVD) on the transposed embedded data
U, S, vh = np.linalg.svd(embedded_data.T)

# Plot the singular values
plt.plot(range(1, len(S) + 1), S, marker='o')
plt.xlabel('Singular Value Index')
plt.ylabel('Singular Value')
plt.title('Singular Value Decomposition')
plt.savefig("SVD")
plt.show()
```

```python
# Plot the first three rows of vh
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.plot(vh[0], vh[1], vh[2], lw=0.5)
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
ax.set_zlabel('Z-axis')
ax.set_title('Lorenz Attractor')
plt.savefig("Lorenz_reconstructed")
plt.show()
```


## Implication for Machine Learning
   
    This implies that $\mathcal{H}_D$ is an embedding of $\mathcal{M}$ which, in turn, means that each vector $\vec{x}$ on $\mathcal{H}_D$ corresponds to a unique vector $\vec{s}$ on $\mathcal{M}$. 
    So $\vec{x}_t$ can be used as a state vector to describe the dynamics of the original system \eqref{eqn:takens_source}. 
 More specifically, an operator $\vec{\Gamma}$ might be introduced, such that:
    \begin{equation}\label{eqn:takens_gamma}
        \vec{x}_{t+1} = \vec{\Gamma}(\vec{x}_t)
    \end{equation}
 where
    \begin{equation}\label{eqn:takens_gamma_expanded}
        \vec{\Gamma} := \vec{F}_D \circ \vec{g} \circ \vec{F}_D^{-1} .
    \end{equation}
    
It is clear from \eqref{eqn:takens_function} that $x^i_{t+1} = \Gamma^i(\vec{x}) = x^{i-1}= u_{t-i}$ for $i \le 2$. 
    The only component which actually needs to be predicted is the first one, i.e.,   
    
    \begin{equation}
        u_{t+1} = x^1_{t+1} 
        = \Gamma^1(\vec{x}_t)
        := \Psi(u_t,u_{t-1}, \dots, u_{t-(D-1)}).
    \end{equation}
    
    
Where $\Psi$ is a function that is able to produce the next element of our time series ($u_{t+1}$) by reading the past $D$ elements -- i.e., the vector $\vec{x}_t$. 
    The function $\Psi$ is guaranteed to exist by Takens's Theorem, in particular by Eq.~\ref{eqn:takens_gamma_expanded}. 

## Linear Model


```python
from sklearn import linear_model

X = embedded_data[:-1,:]
Y = embedded_data[1:,-1]

linear_predictor = linear_model.Ridge(alpha = 1e-5)

linear_predictor.fit(X,Y)

```


```python
x_start=X[-1,:]
x_start = x_start[ np.newaxis,:]
L_seq = 900
x = x_start

preds = []
for _ in range(L_seq):
    pred = linear_predictor.predict(x)
    preds.append(pred)
    x = np.concatenate((x[:,1:],pred[np.newaxis,:]), axis=1)
```



## Neural Network

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt

# Convert NumPy arrays to PyTorch tensors
X_tensor = torch.tensor(X, dtype=torch.float32)
Y_tensor = torch.tensor(Y, dtype=torch.float32)

# Create a simple neural network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Set seeds for reproducibility
seed = 42
np.random.seed(seed)
torch.manual_seed(seed)


# Hyperparameters
input_dim = X.shape[1]
hidden_dim = 128
learning_rate = 1e-4
num_epochs = 100
batch_size = 32


# Create the neural network model
model = NeuralNetwork(input_dim, hidden_dim)

# Loss function and optimizer
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Create a DataLoader for batching
dataset = TensorDataset(X_tensor, Y_tensor)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Lists to store loss history
loss_history = []

# Training loop
for epoch in range(num_epochs):
    for inputs, targets in dataloader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, targets.view(-1, 1))
        loss.backward()
        optimizer.step()

    if epoch % 10 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item()}")

    # Append the current loss to the loss history list
    loss_history.append(loss.item())
```

%%% FIGURE ON LOSS

%%% FIGURE ON TRAINING


```python
x_start=X[-1,:]
x_start = x_start[ np.newaxis,:]
x_start=torch.tensor(x_start, dtype=torch.float32)
L_seq = 1500
x = x_start

nn_preds = []
for _ in range(L_seq):
    pred = model(x)
    nn_preds.append(pred.detach().numpy()[0])
    x = torch.concatenate((x[:,1:],pred), axis=1)
```



### Conclusions
    In real-world applications, it is impossible to check whether the conditions of Takens's Theorem are fulfilled and the dimension $d$ is not known, so that it may appear that it has few practical implications.
    Yet, the theoretical value of this result is high.  
    In fact, theorem~\ref{thm:takens} provides a formal justification for the autoregressive models (introduced in \citep{yule1927method}), which aim at predicting the future of time series from past observations.
