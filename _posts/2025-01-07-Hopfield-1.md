---
layout: post
title: "Hopfield Networks - Part 1"
tags: hopfield, neural_networks
category: machine_learning
---


As the latest Nobel Prize for physics was assigned to John Hopfield, I thought it would be a good idea to write a series of posts about them.
In this first post, I will introduce the Hopfield networks and their main characteristics, giving examples in `Python` of how to implement them.


### Introduction: associative memory

_Associative memory_ is a remarkable feature of the brain that allows us to retrieve information from partial or noisy cues.
Formally, the problem of associative memory can be  stated as follow:

>Store a set of $p$ pattern $x^\mu_i$ in a newtork in such a way that when the network is presented with a new pattern $z_i$, it responds by producing the pattern among the stored one that is most similar to it.

The index $\mu$ labels the patterns ($\mu = 1, 2, \ldots, p$), while the index $i$ the components of each patterns ($i = 1, 2, \ldots, N$).
We will only consider binary patterns, i.e., each component of the patterns can take only two values: $0$ or $1$.
This task appears to be pretty easy: we can simply compute the similarity between the new pattern and the stored (for example, by using the Hamming distance [[^1]]) ones and select the most similar one.
But what we would like to have is having a neural network that is able to perform this task. This is where the Hopfield networks come into play.


Practically, imagine that we have a newtork of $N$ neurons (the same number of components of the patterns): this neurons are connected to each other and they can change their state based on some rules that we can define. 
The task can then be addressed in the following way: given that the network starts from a configuration, the network should evolve in such a way that it reaches a configuration that is one of the stored patterns and it is the closest to the new pattern.


### Formalization of the problem

For mathematical convenience, we use a model in which the state of the networks are still binary, but equal to $-1$ and $+1$ instead of $0$ and $1$.
We will denote the state of each neuron with $S_i$ where $i = 1, 2, \ldots, N$.
Since these neurons are connected among each other, we will encode the strength of this connection is a matrix with elements $w_{ij}$.
The network evolves in time according to the following rule:

\begin{equation}
    S_i(t+1) = \text{sgn}\left(\sum_{j} w_{ij} S_j(t)\right)
\end{equation}

In which the function $\text{sgn}$ is defined as:

\begin{equation}
    \text{sign}(x) = \begin{cases}
        +1 & \text{if } x \geq 0 \\
        -1 & \text{if } x < 0
    \end{cases}
\end{equation}

Now, this update can be conducted in two ways: _synchronous_ or _asynchronous_.
In the _synchronous_ case, all the neurons are updated at the same time, while in the _asynchronous_ case, the neurons are updated one at a time, with the neuron chosen at random.
In this post, we will focus on the _synchronous_ case, as it is easier to implement. From the point of view of the theoretical analysis, they are basically identical.

We can already start implementing those elements in `Python`.
Let's start by generating a random pattern.

```python
# create a random binary pattern
bin_pattern = np.array([1,0,1])
```

As we said, we will use the binary patterns, but we will encode them as $-1$ and $+1$.

```python
def ztm(pattern): # zeros to minus ones
    return np.where(pattern == 0, -1, pattern)

def mtz(pattern): # minus ones to zeros
    return np.where(pattern == -1, 0, pattern)

pattern = ztm(bin_pattern)
```
    [ 1 -1  1]


### One pattern

Let's start by considering the case in which we have only one pattern.
In this case, we want the network to converge to the pattern itself, which needs to be stable.
The stability consists in the fact that the pattern is an equilibrium point of the dynamics, i.e., if we start from the pattern, we will remain there. Formally:

$$
\begin{equation}
    S_i(t+1) = S_i(t)  \quad \forall i
\end{equation}
$$

Since our pattern is $x_i$ (we drop the $\mu$ index since we have only one pattern), the condition for the stability of the pattern is that:

$$
\begin{equation}
    x_i = \text{sgn}\left(\sum_{j} w_{ij} x_j\right) \quad \forall i
\end{equation}
$$

One can see that this condition is satisfied if the element $w_{ij}$ is proportional to the product $x_i x_j$ (as $x_j^2 = 1$).
It makes sense (for reasons that will be clear soon) to take the constant of proportionality to be $1/N$, so that the matrix $w_{ij}$ is defined as:

$$
\begin{equation}
    w_{ij} = \frac{1}{N} \sum_{\mu} x_i^\mu x_j^\mu
\end{equation}
$$

In fact:

$$
\begin{equation}
    x_i = \text{sgn}\left(\sum_{j} \frac{1}{N} x_i x_j x_j\right) = \text{sgn}\left(\frac{1}{N} x_i \sum_{j} x_j x_j\right) = \text{sgn}\left(\frac{1}{N} x_i N\right) = x_i
\end{equation}
$$
which explains the choice of the constant of proportionality.

In `python`, this can easily be implemented using the `np.outer` function.

```python
weights = np.outer(pattern, pattern)
print(weights)
```
    [[ 1 -1  1]
     [-1  1 -1]
     [ 1 -1  1]]

We can now verify that the pattern is stable by defining a `step` function that updates the state of the network according to the rule we defined above.

```python
def step(pattern, weights):
    return np.sign(np.dot(weights, pattern)).astype(np.int64)

print(mtz(step(pattern, weights)))
```
    [ 1 0  1]

As expected, applying the `step` function to the pattern, we obtain the same pattern as output (i.e., _the pattern is stable_).
But as we said, we also require that the pattern is an attractor: if we start from a pattern that is close to the original pattern, the network should converge to the original pattern.
To see that, let's consider an initial state (which has the same shape as the pattern) that is different from the original pattern.
We will call it $k_i$. Let's see what happens when the network evolves  starting from this state.

$$
\begin{equation}
\text{sgn}\left(\sum_{j} w_{ij} k_j\right) = \text{sgn}\left(\sum_{j} \frac{1}{N}x_i x_j  k_j\right) =
\text{sgn}\left(\frac{1}{N} x_i \sum_{j} x_j k_j\right)
\end{equation}
$$

We can now notice that the sum can be divided into the neurons $k_j$ that have the same values as the stored pattern (i.e., they are _correct_) and the one that are in the opposite state (_incorrect_).
The sum can be then written as:

$$
\begin{equation}
\text{sgn}\left(\sum_{j} w_{ij} k_j\right) =
\text{sgn}\left(\frac{1}{N} x_i ( \underbrace{\sum_{j=\text{correct}} x_j k_j + \sum_{j=\text{incorrect}} x_j k_j }_{A})\right)
\end{equation}
$$

So, if the number of correct neurons is greater than the number of incorrect neurons, all the terms of $A$ together will lead to a positive contribution, and the sign of the result will be given by $x_i$, which is the value of the stored pattern.
Let's see this:

```python
new_pattern = ztm(np.array([1,0,0]))
print(step(new_pattern, weights))
```
    [ 1 0  1]

```python
another_new_pattern = ztm(np.array([0,0,1]))
print(mtz(step(another_new_pattern, weights)))
```
    [ 1 0  1]

Yet, if we start from a pattern that has more than one element different from the original pattern, the network will not converge to the original pattern.

```python
    different_pattern = ztm(np.array([0,1,0]))
    print(mtz(step(different_pattern, weights)))
```
    [ 0 1  0]

Indeed, the network converges to a different pattern, which in this case is the negation (all the values flipped) of the original pattern.
We can verify that this pattern is also stable:

```python
neg_pattern = ztm(np.array([0,1,0]))
print(mtz(step(neg_pattern, weights)))
```
    [ 0 1  0]

So in a sense, the network has learned two memories: the original pattern and its negation. This is due to the simmetry of $W$.
Each initial state will converge two the closest (in the sense of Hamming distance) of these two patterns.


### Multiple Patterns

Let's now consider the case in which we have $p$ patterns. As we already said, we would like the network to converge to one of the stored patterns when we start from an initial state that is close to it.
Let's start by generating a set of random patterns:

```python
n_patterns = 4
size = 5
bin_patterns = np.random.randint(2, size=n_patterns*size).reshape(n_patterns, size)
print(bin_patterns)
``` 
    [[0 1 0 0 0]
     [1 0 0 0 1]
     [0 0 0 0 1]
     [0 1 1 1 0]]

For simplicity, we stored them in a matrix in which the rows are the patterns.
In analogy to the case of one pattern, in which we build the matrix as the outer product of the pattern with itself, we can try building the matrix $W$ as the superposition these outer product, which is:

$$
\begin{equation}\label{eqn:hebbian_w}
    w_{ij} = \frac{1}{N} \sum_{\mu}^p x_i^\mu x_j^\mu
\end{equation}
$$

In `python`: 
```python
# remember to convert them
patterns = ztm(bin_patterns)

W = np.zeros((size, size))
for pattern in patterns:
    W += np.outer(pattern, pattern)
W /= size
print(W)
```
    [[ 0.8 -0.4  0.   0.   0.4]
     [-0.4  0.8  0.4  0.4 -0.8]
     [ 0.   0.4  0.8  0.8 -0.4]
     [ 0.   0.4  0.8  0.8 -0.4]
     [ 0.4 -0.8 -0.4 -0.4  0.8]]

This is a form of the [Hebbian learning rule](https://en.wikipedia.org/wiki/Hebbian_theory), which  state that the strength of the connection between two neurons should be proportional to the correlation [[^2]] between their activities.
In fact, to make that explicit we can see that $W$ can directly be computed using:

```python
#cool way
W_2 = 1/size*(patterns.T@patterns)
print(W_2)
```
    [[ 0.8 -0.4  0.   0.   0.4]
     [-0.4  0.8  0.4  0.4 -0.8]
     [ 0.   0.4  0.8  0.8 -0.4]
     [ 0.   0.4  0.8  0.8 -0.4]
     [ 0.4 -0.8 -0.4 -0.4  0.8]]

This can be done in a more compact way using the `np.einsum` function:
```python
# cooler_way
W_1 = np.einsum('ij,ik->jk', patterns, patterns) / size 
print(W_1)
```
    [[ 0.8 -0.4  0.   0.   0.4]
     [-0.4  0.8  0.4  0.4 -0.8]
     [ 0.   0.4  0.8  0.8 -0.4]
     [ 0.   0.4  0.8  0.8 -0.4]
     [ 0.4 -0.8 -0.4 -0.4  0.8]]


We can now verify that the patterns are fixed points by using our `step` function.

```python
print(mtz(patterns[0]))
print(mtz(step(patterns[0],W)))
```
    [0 1 0 0 0]
    [0 1 0 0 0]

### Stability of the patterns

Let's now study the stability of a specific pattern $x^\mu$. 
The stability condition requires:

\begin{equation} 
    \label{eqn:stability}
    x_i^\nu = \text{sgn}\left(h^\nu_i\right) \quad \forall i
\end{equation}

where the input $h^\nu_i$ is defined as:

$$
\begin{equation}
    h^\nu_i = \sum_{j} w_{ij} x_j^\nu =  \frac{1}{N} \sum_{j}\sum_{\mu} x_i^\mu x_j^\mu x_j^\nu
\end{equation}
$$

we can now separate the sum over $\mu$ in two parts: one in which $\mu = \nu$ and one in which $\mu \neq \nu$.

$$
\begin{equation}
    h^\nu_i = x_i^\nu  + \underbrace{\frac{1}{N}\sum_{j}\sum_{\mu \neq \nu} x_i^\mu x_j^\mu x_j^\nu}_{\text{Cross Talk}}
\end{equation}
$$

If the second term were zero, the stability condition \eqref{eqn:stability} would be satisfied.
This is also true if the second term is not zero, but it is small enough: if its magnitude is smaller than 1,  it will not change the sign of $h_i^\nu$.
```python
pattern_pert = ztm(np.array([1,1,0,0,0]))
print(mtz(step(pattern_pert, W)))
```
    [0 1 0 0 0]
### Storage Capacity
To see how many patterns can be stored in a network, let's consider the following quantity:

$$
\begin{equation}
    C^\nu_i := -x^\nu_i \frac{1}{N}\sum_{j}\sum_{\mu \neq \nu} x_i^\mu x_j^\mu x_j^\nu
\end{equation}
$$

Which is the Cross Talk term in the stability condition multiplied by $-x_i^\nu$. 
If $C^\nu_i$ is negative, the Cross Talk term has the same sign of the pattern element $x_i^\nu$ and it will do nothing.
If instead $C^\nu_i$ is positive and larger than 1, it changes the sign of $h_i^\nu$ and the unit $i$ of the pattern $\nu$ will not be stable.
$C^\nu_i$ only depends on the patterns $x_j^\mu$ that are stored in the network:
if we consider purely random patterns, with equal probability of $-1$ and $+1$, we can estimate the probability of errors:

$$
P_{\text{error}} = P(C^\nu_i > 1)
$$

So, if we decide an error rate that we can tolerate (e.g., $P_\text{error} < 0.01$), we can estimate the number of patterns that can be stored in the network. 
To do so, let's compute $P_{\text{error}}$ in the case of large $N$ and $p$. As $C^\nu_i$ is $\frac{1}{N}$ times the sum of $N(p-1) \approx Np$ independent random variables that can be either $-1$ or $1$, we know that its distribution will be binomial with mean $0$ and $\sigma^2 = p/N$.
But since we assumed $Np$ to be large, we can approximate it with a gaussian. So we can write: 

$$
\begin{equation}
P_{\text{error}} \approx \int_{1}^{\infty} \frac{1}{\sqrt{2\pi p/N}}e^{-x^2/2p/N}dx = \frac{1}{2}\left(1 - \text{erf}({\sqrt{\frac{N}{2p}}})\right)
\end{equation}
$$

This equation allows us to estimate the number of patterns that can be stored in the network.
As one could have expected, the number of patterns $p$ depends on the number of neurons $N$, so it usually make sense to consider the ratio $\alpha := p/N$.







------------------------------------------------------

[^1]: The Hamming distance between two patterns is defined as the number of components in which they differ.
[^2]: You can convince yourself that if you have and array $X$ of $p$ data points of dimension $N$, the covariance matrix is given by $\frac{1}{p}X^T X$. In our case we are doing the same but dropping the $p$ factor and replacing it with $N$. This doesn't really matter, as the only thing we need is the proportionality. 