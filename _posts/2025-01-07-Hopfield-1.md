---
layout: post
title: "Hopfield Networks - Part 1"
tags: hopfield, neural_networks
category: machine_learning
---


As the latest Nobel Prize for physics was assigned to John Hopfield, I thought it would be a good idea to write a series of posts about them.
In this first post, I will introduce the Hopfield networks and their main characteristics, giving examples in `Python` of how to implement them.


### Introduction: associative memory

_Associative memory_ is a remarkable feature of the brain that allows us to retrieve information from partial or noisy cues.
Formally, the problem of associative memory can be  stated as follow:

>Store a set of $p$ pattern $x^\mu_i$ in a newtork in such a way that when the network is presented with a new pattern $z_i$, it responds by producing the pattern among the stored one that is most similar to it.

The index $\mu$ labels the patterns ($\mu = 1, 2, \ldots, p$), while the index $i$ the components of each patterns ($i = 1, 2, \ldots, N$).
We will only consider binary patterns, i.e., each component of the patterns can take only two values: $0$ or $1$.
This task appears to be pretty easy: we can simply compute the similarity between the new pattern and the stored (for example, by using the Hamming distance [[^1]]) ones and select the most similar one.
But what we would like to have is having a neural network that is able to perform this task. This is where the Hopfield networks come into play.


Practically, imagine that we have a newtork of $N$ neurons (the same number of components of the patterns): this neurons are connected to each other and they can change their state based on some rules that we can define. 
The task can then be addressed in the following way: given that the network starts from a configuration, the network should evolve in such a way that it reaches a configuration that is one of the stored patterns and it is the closest to the new pattern.


### Formalization of the problem

For mathematical convenience, we use a model in which the state of the networks are still binary, but equal to $-1$ and $+1$ instead of $0$ and $1$.
We will denote the state of each neuron with $S_i$ where $i = 1, 2, \ldots, N$.
Since these neurons are connected among each other, we will encode the strength of this connection is a matrix with elements $w_{ij}$.
The network evolves in time according to the following rule:

\begin{equation}
    S_i(t+1) = \text{sgn}\left(\sum_{j} w_{ij} S_j(t)\right)
\end{equation}

In which the function $\text{sgn}$ is defined as:

\begin{equation}
    \text{sign}(x) = \begin{cases}
        +1 & \text{if } x \geq 0 \\
        -1 & \text{if } x < 0
    \end{cases}
\end{equation}

Now, this update can be conducted in two ways: _synchronous_ or _asynchronous_.
In the _synchronous_ case, all the neurons are updated at the same time, while in the _asynchronous_ case, the neurons are updated one at a time, with the neuron chosen at random.
In this post, we will focus on the _synchronous_ case, as it is easier to implement. From the point of view of the theoretical analysis, they are basically identical.

We can already start implementing those elements in `Python`.
Let's start by generating a random pattern.

```python
# create a random binary pattern
bin_pattern = np.array([1,0,1])
```

As we said, we will use the binary patterns, but we will encode them as $-1$ and $+1$.

```python
def ztm(pattern): # zeros to minus ones
    return np.where(pattern == 0, -1, pattern)

def mtz(pattern): # minus ones to zeros
    return np.where(pattern == -1, 0, pattern)

pattern = ztm(bin_pattern)
```
    [ 1 -1  1]


### One pattern

Let's start by considering the case in which we have only one pattern.
In this case, we want the network to converge to the pattern itself, which needs to be stable.
The stability consists in the fact that the pattern is an equilibrium point of the dynamics, i.e., if we start from the pattern, we will remain there. Formally:

$$
\begin{equation}
    S_i(t+1) = S_i(t)  \quad \forall i
\end{equation}
$$

Since our pattern is $x_i$ (we drop the $\mu$ index since we have only one pattern), the condition for the stability of the pattern is that:

$$
\begin{equation}
    x_i = \text{sgn}\left(\sum_{j} w_{ij} x_j\right) \quad \forall i
\end{equation}
$$

One can see that this condition is satisfied if the element $w_{ij}$ is proportional to the product $x_i x_j$ (as $x_j^2 = 1$).
It makes sense (for reasons that will be clear later) to take the constant of proportionality to be $1/N$, so that the matrix $w_{ij}$ is defined as:

$$
\begin{equation}
    w_{ij} = \frac{1}{N} \sum_{\mu} x_i^\mu x_j^\mu
\end{equation}
$$

In python, this can easily be implemented using the `np.outer` function.

```python
weights = np.outer(pattern, pattern)
print(weights)
```
    [[ 1 -1  1]
     [-1  1 -1]
     [ 1 -1  1]]

We can now verify that the pattern is stable by defining a `step` function that updates the state of the network according to the rule we defined above.

```python
def step(pattern, weights):
    return np.sign(np.dot(weights, pattern)).astype(np.int64)

print(mtz(step(pattern, weights)))
```
    [ 1 0  1]

As expected, applying the `step` function to the pattern, we obtain the same pattern as output (i.e., the pattern is stable).

We can also note that if we switch of the pattern elements, the network will still converge to the pattern.

```python
new_pattern = ztm(np.array([1,0,0]))
print(step(new_pattern, weights))
```
    [ 1 0  1]

```python
another_new_pattern = ztm(np.array([0,0,1]))
print(mtz(step(another_new_pattern, weights)))
```
    [ 1 0  1]

Yet, if we start from a pattern that has more than one element different from the original pattern, the network will not converge to the original pattern.

```python
    different_pattern = ztm(np.array([0,1,0]))
    print(mtz(step(different_pattern, weights)))
```
    [ 0 1  0]

Indeed, the network converges to a different pattern, which in this case is the negation (all the values flipped) of the original pattern.
We can verify that this pattern is stable

```python
neg_pattern = ztm(np.array([0,1,0]))
print(mtz(step(neg_pattern, weights)))
```
    [ 0 1  0]


### Multiple Patterns

```python
n_patterns = 4
size = 5
bin_patterns = np.random.randint(2, size=n_patterns*size).reshape(n_patterns, size)
print(bin_patterns)
``` 
    [[0 1 0 0 0]
     [1 0 0 0 1]
     [0 0 0 0 1]
     [0 1 1 1 0]]


$$
\begin{equation}\label{eqn:hebbian_w}
    w_{ij} = \frac{1}{N} \sum_{\mu}^p x_i^\mu x_j^\mu
\end{equation}
$$


```python
# remember to convert them!!!
patterns = ztm(bin_patterns)

W = np.zeros((size, size))
for pattern in patterns:
    W += np.outer(pattern, pattern)
W /= n_patterns
print(W)
```
    [[ 1.  -0.5  0.   0.   0.5]
     [-0.5  1.   0.5  0.5 -1. ]
     [ 0.   0.5  1.   1.  -0.5]
     [ 0.   0.5  1.   1.  -0.5]
     [ 0.5 -1.  -0.5 -0.5  1. ]]


```python
# cooler_way
W_1 = np.einsum('ij,ik->jk', patterns, patterns) / n_patterns
print(W_1)
```
    [[ 1.  -0.5  0.   0.   0.5]
     [-0.5  1.   0.5  0.5 -1. ]
     [ 0.   0.5  1.   1.  -0.5]
     [ 0.   0.5  1.   1.  -0.5]
     [ 0.5 -1.  -0.5 -0.5  1. ]]

```python
print(mtz(patterns[0]))
print(mtz(step(patterns[0],W)))
```
    [0 1 0 0 0]
    [0 1 0 0 0]

### Stability of the patterns

Let's now study the stability of a specific pattern $x^\mu$. 
The stability condition requires:
$$
\begin{equation} \label{eqn:stability}
    x_i^\nu = \text{sgn}\left(h^\nu_i\right) \quad \forall i
\end{equation}
$$


where the input $h^\nu_i$ is defined as:

$$
\begin{equation}
    h^\nu_i = \sum_{j} w_{ij} x_j^\nu =  \frac{1}{N} \sum_{j}\sum_{\mu} x_i^\mu x_j^\mu x_j^\nu
\end{equation}
$$

we can now separate the sum over $\mu$ in two parts: one in which $\mu = \nu$ and one in which $\mu \neq \nu$.

$$
\begin{equation}
    h^\nu_i = x_i^\nu  + \underbrace{\frac{1}{N}\sum_{j}\sum_{\mu \neq \nu} x_i^\mu x_j^\mu x_j^\nu}_{\text{Cross Talk}}
\end{equation}
$$

If the second term were zero, the stability condition \eqref{eqn:stability} would be satisfied.
This is also true if the second term is not zero, but it is small enough: if its magnitude is smaller than 1,  it will not change the sign of $h_i^\nu$.



```python
pattern_pert = ztm(np.array([1,1,0,0,0]))
print(mtz(step(pattern_pert, W)))
```
    [0 1 0 0 0]



### Storage Capacity

$$
\begin{equation}
    C^\nu_i := -x^\nu_i \frac{1}{N}\sum_{j}\sum_{\mu \neq \nu} x_i^\mu x_j^\mu x_j^\nu
\end{equation}
$$












------------------------------------------------------

[^1]: The Hamming distance between two patterns is defined as the number of components in which they differ.